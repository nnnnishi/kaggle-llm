#!/usr/bin/env python3
"""
3.5 å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«
ã€Kaggle ã§ã¯ã˜ã‚ã‚‹å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«å…¥é–€ã€ç¬¬3ç« 

Gemmaã‚’ä½¿ã£ãŸãƒ†ã‚­ã‚¹ãƒˆåˆ†é¡ï¼ˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ™ãƒ¼ã‚¹ï¼‰
"""

import os
import numpy as np
import pandas as pd
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
from tqdm import tqdm
import re
import warnings
warnings.filterwarnings('ignore')

# =============================================================================
# è¨­å®š
# =============================================================================
print("=" * 60)
print("3.5 å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ï¼ˆGemmaï¼‰")
print("=" * 60)

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"\nğŸ–¥ï¸ Device: {device}")
if device.type == 'cuda':
    print(f"   GPU: {torch.cuda.get_device_name(0)}")
    print(f"   VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB")

# ãƒ‡ãƒ¼ã‚¿ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
if os.path.exists('/content/kaggle-llm/data'):
    DATA_DIR = '/content/kaggle-llm/data'
elif os.path.exists('/root/kaggle-llm/data'):
    DATA_DIR = '/root/kaggle-llm/data'
else:
    DATA_DIR = 'data'

# è¨­å®š
MODEL_NAME = 'google/gemma-2-2b-jpn-it'  # æ—¥æœ¬èªå¯¾å¿œGemma
MAX_NEW_TOKENS = 16
BATCH_SIZE = 1  # LLMã¯1ã‚µãƒ³ãƒ—ãƒ«ãšã¤å‡¦ç†

# ã‚µãƒ³ãƒ—ãƒ«æ•°åˆ¶é™ï¼ˆãƒ‡ãƒ¢ç”¨ï¼‰
SAMPLE_LIMIT = 100  # ãƒ•ãƒ«ãƒ‡ãƒ¼ã‚¿ã®å ´åˆã¯Noneã«è¨­å®š

# =============================================================================
# ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ
# =============================================================================
PROMPT_TEMPLATE = """You are a helpful assistant that classifies product reviews.

Given the following product review, predict the rating from 1 to 5 stars.
- 1 star: Very negative review
- 2 stars: Negative review
- 3 stars: Neutral review
- 4 stars: Positive review
- 5 stars: Very positive review

Review: {review}

Respond with ONLY a single number from 1 to 5.
Rating:"""

# =============================================================================
# LLMæ¨è«–é–¢æ•°
# =============================================================================
def predict_with_llm(model, tokenizer, texts, batch_size=1):
    """LLMã‚’ä½¿ã£ã¦è©•ä¾¡ã‚’äºˆæ¸¬"""
    predictions = []
    
    for text in tqdm(texts, desc='LLM Predicting'):
        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆä½œæˆ
        prompt = PROMPT_TEMPLATE.format(review=text[:500])  # é•·ã™ãã‚‹ãƒ†ã‚­ã‚¹ãƒˆã¯åˆ‡ã‚Šè©°ã‚
        
        # ãƒˆãƒ¼ã‚¯ãƒ³åŒ–
        inputs = tokenizer(
            prompt,
            return_tensors='pt',
            truncation=True,
            max_length=512
        ).to(device)
        
        # ç”Ÿæˆ
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=MAX_NEW_TOKENS,
                do_sample=False,  # æ±ºå®šçš„ç”Ÿæˆ
                pad_token_id=tokenizer.eos_token_id,
            )
        
        # ãƒ‡ã‚³ãƒ¼ãƒ‰
        generated = tokenizer.decode(outputs[0], skip_special_tokens=True)
        
        # RatingæŠ½å‡º
        rating = extract_rating(generated)
        predictions.append(rating)
    
    return np.array(predictions)

def extract_rating(text):
    """ç”Ÿæˆã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰Ratingã‚’æŠ½å‡º"""
    # "Rating:" ã®å¾Œã®æ•°å­—ã‚’æ¢ã™
    match = re.search(r'Rating:\s*(\d)', text)
    if match:
        rating = int(match.group(1))
        if 1 <= rating <= 5:
            return rating
    
    # æœ€å¾Œã®æ•°å­—ã‚’æ¢ã™
    numbers = re.findall(r'\b([1-5])\b', text)
    if numbers:
        return int(numbers[-1])
    
    # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤
    return 3

# =============================================================================
# ãƒ¡ã‚¤ãƒ³å‡¦ç†
# =============================================================================
def main():
    # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
    print("\nğŸ“ ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿...")
    train = pd.read_csv(f'{DATA_DIR}/train.csv')
    test = pd.read_csv(f'{DATA_DIR}/test.csv')
    
    train['Review Text'] = train['Review Text'].fillna('')
    test['Review Text'] = test['Review Text'].fillna('')
    
    print(f"   train: {train.shape}")
    print(f"   test:  {test.shape}")
    
    # ã‚µãƒ³ãƒ—ãƒ«åˆ¶é™ï¼ˆãƒ‡ãƒ¢ç”¨ï¼‰
    if SAMPLE_LIMIT:
        print(f"\nâš ï¸ ãƒ‡ãƒ¢ãƒ¢ãƒ¼ãƒ‰: {SAMPLE_LIMIT}ã‚µãƒ³ãƒ—ãƒ«ã®ã¿ä½¿ç”¨")
        train_sample = train.head(SAMPLE_LIMIT)
        test_sample = test.head(SAMPLE_LIMIT)
    else:
        train_sample = train
        test_sample = test
    
    # ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿
    print(f"\nğŸ¤– ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿: {MODEL_NAME}")
    print("   ï¼ˆåˆå›ã¯ãƒ¢ãƒ‡ãƒ«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã«æ™‚é–“ãŒã‹ã‹ã‚Šã¾ã™ï¼‰")
    
    try:
        tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
        model = AutoModelForCausalLM.from_pretrained(
            MODEL_NAME,
            torch_dtype=torch.float16,  # ãƒ¡ãƒ¢ãƒªç¯€ç´„
            device_map='auto',
        )
        print("   âœ… ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿å®Œäº†")
    except Exception as e:
        print(f"   âŒ ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
        print("\nğŸ’¡ Hugging Face ãƒˆãƒ¼ã‚¯ãƒ³ãŒå¿…è¦ãªå ´åˆãŒã‚ã‚Šã¾ã™:")
        print("   1. https://huggingface.co/settings/tokens ã§ãƒˆãƒ¼ã‚¯ãƒ³å–å¾—")
        print("   2. huggingface-cli login ã‚’å®Ÿè¡Œ")
        print("   3. Gemmaã®åˆ©ç”¨è¦ç´„ã«åŒæ„")
        return
    
    # =============================================================================
    # æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ã§ç²¾åº¦ç¢ºèª
    # =============================================================================
    print("\n" + "=" * 60)
    print("ğŸ” æ¤œè¨¼ï¼ˆè¨“ç·´ãƒ‡ãƒ¼ã‚¿ã®ã‚µãƒ³ãƒ—ãƒ«ï¼‰")
    print("=" * 60)
    
    val_texts = train_sample['Review Text'].values
    val_labels = train_sample['Rating'].values
    
    val_preds = predict_with_llm(model, tokenizer, val_texts)
    
    # ç²¾åº¦è¨ˆç®—
    from sklearn.metrics import accuracy_score, f1_score, classification_report
    
    acc = accuracy_score(val_labels, val_preds)
    f1 = f1_score(val_labels, val_preds, average='macro')
    
    print(f"\nğŸ“Š æ¤œè¨¼çµæœ:")
    print(f"   Accuracy: {acc:.4f}")
    print(f"   Macro F1: {f1:.4f}")
    print(f"\nğŸ“‹ Classification Report:")
    print(classification_report(val_labels, val_preds, digits=4))
    
    # =============================================================================
    # ãƒ†ã‚¹ãƒˆäºˆæ¸¬
    # =============================================================================
    print("\n" + "=" * 60)
    print("ğŸ”® ãƒ†ã‚¹ãƒˆäºˆæ¸¬")
    print("=" * 60)
    
    test_texts = test_sample['Review Text'].values
    test_preds = predict_with_llm(model, tokenizer, test_texts)
    
    print(f"\nğŸ“Š ãƒ†ã‚¹ãƒˆäºˆæ¸¬åˆ†å¸ƒ:")
    print(pd.Series(test_preds).value_counts().sort_index())
    
    # ä¿å­˜
    submission = pd.DataFrame({
        'id': range(len(test_preds)),
        'Rating': test_preds
    })
    submission_path = f'{DATA_DIR}/submission_llm.csv'
    submission.to_csv(submission_path, index=False)
    print(f"\nğŸ’¾ ä¿å­˜: {submission_path}")
    
    # =============================================================================
    # å®Œäº†
    # =============================================================================
    print("\n" + "=" * 60)
    print("âœ… 3.5 LLM å®Œäº†!")
    print("=" * 60)
    print(f"""
ã€è£œè¶³ã€‘
- å®Œå…¨ãªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§å®Ÿè¡Œã™ã‚‹å ´åˆã¯ SAMPLE_LIMIT = None ã«è¨­å®š
- ã‚ˆã‚Šå¤§ããªãƒ¢ãƒ‡ãƒ«ï¼ˆgemma-7bç­‰ï¼‰ã‚’ä½¿ã†ã¨ç²¾åº¦å‘ä¸Šã®å¯èƒ½æ€§ã‚ã‚Š
- ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®æœ€é©åŒ–ã§ã‚‚ç²¾åº¦å‘ä¸Šå¯èƒ½

ã€æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—ã€‘
- 3.6: è¤‡æ•°ãƒ¢ãƒ‡ãƒ«ã®ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«
""")

if __name__ == '__main__':
    main()
