#!/usr/bin/env python3
"""
3.6 ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«
ã€Kaggle ã§ã¯ã˜ã‚ã‚‹å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«å…¥é–€ã€ç¬¬3ç« 

è¤‡æ•°ãƒ¢ãƒ‡ãƒ«ã®äºˆæ¸¬ã‚’çµ„ã¿åˆã‚ã›ã¦æœ€çµ‚äºˆæ¸¬ã‚’ä½œæˆ
"""

import os
import numpy as np
import pandas as pd
from sklearn.metrics import accuracy_score, f1_score, classification_report
import warnings
warnings.filterwarnings('ignore')

# =============================================================================
# è¨­å®š
# =============================================================================
print("=" * 60)
print("3.6 ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«")
print("=" * 60)

# ãƒ‡ãƒ¼ã‚¿ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
if os.path.exists('/content/kaggle-llm/data'):
    DATA_DIR = '/content/kaggle-llm/data'
elif os.path.exists('/root/kaggle-llm/data'):
    DATA_DIR = '/root/kaggle-llm/data'
else:
    DATA_DIR = 'data'

# =============================================================================
# äºˆæ¸¬ãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿
# =============================================================================
print("\nğŸ“ äºˆæ¸¬ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿...")

predictions = {}
available_models = []

# TF-IDFäºˆæ¸¬
tfidf_path = f'{DATA_DIR}/submission_tfidf.csv'
if os.path.exists(tfidf_path):
    predictions['tfidf'] = pd.read_csv(tfidf_path)['Rating'].values
    available_models.append('tfidf')
    print(f"   âœ… TF-IDF: {tfidf_path}")
else:
    print(f"   âš ï¸ TF-IDF: ãƒ•ã‚¡ã‚¤ãƒ«ãªã—")

# BERTäºˆæ¸¬
bert_path = f'{DATA_DIR}/submission_bert.csv'
if os.path.exists(bert_path):
    predictions['bert'] = pd.read_csv(bert_path)['Rating'].values
    available_models.append('bert')
    print(f"   âœ… BERT: {bert_path}")
else:
    print(f"   âš ï¸ BERT: ãƒ•ã‚¡ã‚¤ãƒ«ãªã—")

# LLMäºˆæ¸¬
llm_path = f'{DATA_DIR}/submission_llm.csv'
if os.path.exists(llm_path):
    predictions['llm'] = pd.read_csv(llm_path)['Rating'].values
    available_models.append('llm')
    print(f"   âœ… LLM: {llm_path}")
else:
    print(f"   âš ï¸ LLM: ãƒ•ã‚¡ã‚¤ãƒ«ãªã—")

# Kaggle Bridgeäºˆæ¸¬
kaggle_path = f'{DATA_DIR}/submission_kaggle.csv'
if os.path.exists(kaggle_path):
    predictions['kaggle'] = pd.read_csv(kaggle_path)['Rating'].values
    available_models.append('kaggle')
    print(f"   âœ… Kaggle: {kaggle_path}")
else:
    print(f"   âš ï¸ Kaggle: ãƒ•ã‚¡ã‚¤ãƒ«ãªã—")

print(f"\nğŸ“Š åˆ©ç”¨å¯èƒ½ãªãƒ¢ãƒ‡ãƒ«: {available_models}")

if len(available_models) < 2:
    print("\nâš ï¸ ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ã«ã¯æœ€ä½2ã¤ã®ãƒ¢ãƒ‡ãƒ«äºˆæ¸¬ãŒå¿…è¦ã§ã™")
    print("   å…ˆã« 3.3, 3.4, 3.5 ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„")
    
    # å˜ä¸€ãƒ¢ãƒ‡ãƒ«ã®äºˆæ¸¬ãŒã‚ã‚Œã°ã€ãã‚Œã‚’æœ€çµ‚äºˆæ¸¬ã¨ã—ã¦ä½¿ç”¨
    if len(available_models) == 1:
        model_name = available_models[0]
        final_preds = predictions[model_name]
        print(f"\n   å˜ä¸€ãƒ¢ãƒ‡ãƒ« ({model_name}) ã®äºˆæ¸¬ã‚’ä½¿ç”¨ã—ã¾ã™")
    else:
        print("   äºˆæ¸¬ãƒ•ã‚¡ã‚¤ãƒ«ãŒã‚ã‚Šã¾ã›ã‚“ã€‚çµ‚äº†ã—ã¾ã™ã€‚")
        exit(1)
else:
    # =============================================================================
    # ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«æ–¹æ³•
    # =============================================================================
    print("\n" + "=" * 60)
    print("ğŸ”„ ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«æ‰‹æ³•")
    print("=" * 60)
    
    # äºˆæ¸¬ã‚’é…åˆ—ã«ã¾ã¨ã‚ã‚‹
    pred_matrix = np.array([predictions[m] for m in available_models])
    n_samples = pred_matrix.shape[1]
    
    # --- æ–¹æ³•1: å¤šæ•°æ±ºï¼ˆVotingï¼‰ ---
    print("\nğŸ“Š æ–¹æ³•1: å¤šæ•°æ±ºï¼ˆHard Votingï¼‰")
    voting_preds = []
    for i in range(n_samples):
        votes = pred_matrix[:, i]
        # æœ€é »å€¤ã‚’å–å¾—ï¼ˆã‚¿ã‚¤ã®å ´åˆã¯å°ã•ã„æ–¹ï¼‰
        unique, counts = np.unique(votes, return_counts=True)
        voting_preds.append(unique[np.argmax(counts)])
    voting_preds = np.array(voting_preds)
    print(f"   äºˆæ¸¬åˆ†å¸ƒ: {pd.Series(voting_preds).value_counts().sort_index().to_dict()}")
    
    # --- æ–¹æ³•2: å¹³å‡ï¼ˆSoft Votingé¢¨ï¼‰ ---
    print("\nğŸ“Š æ–¹æ³•2: å¹³å‡")
    avg_preds = np.round(pred_matrix.mean(axis=0)).astype(int)
    avg_preds = np.clip(avg_preds, 1, 5)  # 1-5ã®ç¯„å›²ã«ã‚¯ãƒªãƒƒãƒ—
    print(f"   äºˆæ¸¬åˆ†å¸ƒ: {pd.Series(avg_preds).value_counts().sort_index().to_dict()}")
    
    # --- æ–¹æ³•3: é‡ã¿ä»˜ãå¹³å‡ ---
    print("\nğŸ“Š æ–¹æ³•3: é‡ã¿ä»˜ãå¹³å‡")
    # ãƒ¢ãƒ‡ãƒ«ã”ã¨ã®é‡ã¿ï¼ˆBERTã‚„LLMã‚’é‡è¦–ï¼‰
    weights = {
        'tfidf': 0.2,
        'bert': 0.4,
        'llm': 0.3,
        'kaggle': 0.1,
    }
    
    # åˆ©ç”¨å¯èƒ½ãªãƒ¢ãƒ‡ãƒ«ã®é‡ã¿ã‚’æ­£è¦åŒ–
    available_weights = np.array([weights.get(m, 0.25) for m in available_models])
    available_weights = available_weights / available_weights.sum()
    
    print(f"   é‡ã¿: {dict(zip(available_models, available_weights.round(3)))}")
    
    weighted_preds = np.zeros(n_samples)
    for i, model in enumerate(available_models):
        weighted_preds += predictions[model] * available_weights[i]
    weighted_preds = np.round(weighted_preds).astype(int)
    weighted_preds = np.clip(weighted_preds, 1, 5)
    print(f"   äºˆæ¸¬åˆ†å¸ƒ: {pd.Series(weighted_preds).value_counts().sort_index().to_dict()}")
    
    # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯é‡ã¿ä»˜ãå¹³å‡
    final_preds = weighted_preds
    
    # =============================================================================
    # å„æ‰‹æ³•ã®ä¸€è‡´åº¦
    # =============================================================================
    print("\n" + "=" * 60)
    print("ğŸ“ ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«æ‰‹æ³•é–“ã®ä¸€è‡´åº¦")
    print("=" * 60)
    
    print(f"   Voting vs Average: {(voting_preds == avg_preds).mean()*100:.1f}%")
    print(f"   Voting vs Weighted: {(voting_preds == weighted_preds).mean()*100:.1f}%")
    print(f"   Average vs Weighted: {(avg_preds == weighted_preds).mean()*100:.1f}%")

# =============================================================================
# æœ€çµ‚äºˆæ¸¬ã®ä¿å­˜
# =============================================================================
print("\n" + "=" * 60)
print("ğŸ’¾ æœ€çµ‚äºˆæ¸¬ã®ä¿å­˜")
print("=" * 60)

# ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿ï¼ˆIDã®ç¢ºèªç”¨ï¼‰
test = pd.read_csv(f'{DATA_DIR}/test.csv')

# æœ€çµ‚æå‡ºãƒ•ã‚¡ã‚¤ãƒ«
submission = pd.DataFrame({
    'id': range(len(final_preds)),
    'Rating': final_preds
})

final_path = f'{DATA_DIR}/submission_ensemble.csv'
submission.to_csv(final_path, index=False)
print(f"\nâœ… æœ€çµ‚äºˆæ¸¬ã‚’ä¿å­˜: {final_path}")

print(f"\nğŸ“Š æœ€çµ‚äºˆæ¸¬åˆ†å¸ƒ:")
print(submission['Rating'].value_counts().sort_index())

# =============================================================================
# OOFäºˆæ¸¬ãŒã‚ã‚Œã°æ¤œè¨¼ã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—
# =============================================================================
oof_bert_path = f'{DATA_DIR}/oof_bert.npy'
if os.path.exists(oof_bert_path):
    print("\n" + "=" * 60)
    print("ğŸ“ˆ OOFæ¤œè¨¼ã‚¹ã‚³ã‚¢ï¼ˆBERTï¼‰")
    print("=" * 60)
    
    train = pd.read_csv(f'{DATA_DIR}/train.csv')
    oof_preds = np.load(oof_bert_path)
    
    acc = accuracy_score(train['Rating'], oof_preds)
    f1 = f1_score(train['Rating'], oof_preds, average='macro')
    
    print(f"   Accuracy: {acc:.4f}")
    print(f"   Macro F1: {f1:.4f}")

# =============================================================================
# å®Œäº†
# =============================================================================
print("\n" + "=" * 60)
print("âœ… 3.6 ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ« å®Œäº†!")
print("=" * 60)
print(f"""
ã€ç”Ÿæˆã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ã€‘
- {final_path}

ã€Kaggleæå‡ºã€‘
1. submission_ensemble.csv ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
2. Kaggleã‚³ãƒ³ãƒšãƒšãƒ¼ã‚¸ã§ Submit

ã€ã•ã‚‰ãªã‚‹æ”¹å–„æ¡ˆã€‘
- Stacking: ãƒ¡ã‚¿ãƒ¢ãƒ‡ãƒ«ã§äºˆæ¸¬ã‚’çµ±åˆ
- Blending: åˆ¥ã®æ¤œè¨¼ã‚»ãƒƒãƒˆã§é‡ã¿ã‚’æœ€é©åŒ–
- ç•°ãªã‚‹ãƒ¢ãƒ‡ãƒ«ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã®è¿½åŠ 
""")
